## Lezione #6

_29/04/2021_

Introduzione al Machine Learning

<small>üö∏ la lezione che state per vedere fa <span style="color:gold"><b>largo</b></span> uso di semplificazioni e abusi di notazione</small>

---
#### Perch√® ne parliamo?

<ul>
    <li>√à il <span style="color:crimson"><b>core</b></span> dei nostri use case<sup>‚ô°</sup></li>
    <li>Trova soluzioni <span style="color:gold"><b>facili</b></span> a problemi <span style="color:gold"><b>difficili</b></span><sup>‚ôß</sup></li>
    <li>Permette di <span style="color:dodgerblue"><b>automatizzare</b></span> task laboriosi</li>
    <li>Fornisce supporto <span style="color:#FF7F50"><b>data driven</b></span> a decisioni aziendali</li>
</ul>
</br>
</br>
<small>
    <ul class="no-bullets">
        <li>‚ô° Anche se <em>meno spesso</em> di quanto vorremmo</li>
        <li>‚ôß Che √® anche un po' il motivo per cui ci hanno assunto</li>
    </ul>
</small>

--
#### Quando possiamo usarlo?

<ul>
    <li>Abbiamo una <span style="color:gold"><b>chiara</b></span> domanda di business in mente</li>
    <li>Legata a quantit√† <span style="color:dodgerblue"><b>misurabili</b></span></li>
    <li>Abbiamo accesso ai <span style="color:#FF7F50"><b>dati</b></span> e possiamo usarli</li>
    <li>Crediamo che la risposta sia <span style="color:crimson"><b>nascosta/contenuta</b></span> nei dati</li>
</ul>

--
#### Quando  <span style="color:crimson">NON</span> possiamo/dobbiamo usarlo?

<ul>
    <li>I dati <span style="color:crimson"><b>non</b></span> esistono o non possiamo accervi<sup>‚ô¢</sup></li>
    <li>In realt√† vorremmo un <span style="color:gold"><b>oracolo</b></span></li>
    <li>Abbiamo un problema deterministico la cui meccanica √® nota<sup>‚ô§</sup></li>
</ul>
</br>
</br>
<small>
    <ul class="no-bullets">
        <li>‚ô¢ Capita <em>pi√π spesso</em> di quanto vorremmo</li>
        <li>‚ô§ Sarebbe un <em>overkill</em></li>
    </ul>
</small>

---
Che cos'√® il <span style="color:#FF7F50"><b>Machine Learning</b></span>?

--
#### definizione per tutti

<blockquote>
a set of methods that can automatically <span style="color:#FF7F50">detect patterns</span> in data, and then
use the uncovered patterns to <span style="color:gold">predict</span> future data, or to perform other kinds of
<span style="color:gold">decision making</span>
</blockquote>

<small>[Kevin P. Murphy, 2012]<small>

--
#### definizione per pochi

<blockquote>
a program <span style="color:#FF7F50">learns</span> from experience <span style="color:#FF7F50">$X$</span> with respect to some task $t$ and some performance
measure $L$, if $L(t)$ <span style="color:gold">improves</span> with <span style="color:gold">$X$</span>
</blockquote>

<small>[Tom Mitchell, 1997]<small>

--

Ma cosa significa <span style="color:#FF7F50"><b>imparare</b></span> dai dati?

---
#### Un po' di nomenclatura

--
##### Dati

 <div class="twocolumn">
    <div>
    <ul>
        <li>Organizzati in matrici $X$ di dimensione $n \times d$</li>
        <li>$n \rightarrow$ esempi (<em>i.e. samples, data point, ...</em>)</li>
        <li>$d \rightarrow$ predittori (<em>i.e. features</em>)</li>
    </ul>
    </div>
    <div>
        <img src="./assets/06/data.png" height="400"/>
    </div>
</div>

--

Ogni _esempio_ √® un <span style="color:dodgerblue"><b>vettore</b></span> $\mathbf{x}_i \in \mathbb{R}^d$

<img src="./assets/06/sample.png" height="150"/>

--
#####  _Problemi non supervisionati_<sup>‚ôô</sup>

- Assenza di altre info
- Esempi:
    - Segmentazione customer base in base agli acquisti $\rightarrow$ <span style="color:crimson"><b>clustering</b></span>
    - Rappresentazione compatta scenari multidimensionali $\rightarrow$ <span style="color:darkcyan"><b>dimensionality reduction</b></span>

<small>‚ôô<em>Unsupervised Machine Learning (focus Lezione 8)</em></small>

--
#####  _Problemi supervisionati_<sup>‚ôï</sup>

- Presenza di una variabile <span style="color:GreenYellow">target $y$</span>
- I valori assunti da <span style="color:GreenYellow">$y$</span> possono essere:
    - continui, <span style="color:GreenYellow">$y_i$</span> $\in \mathbb{R} \rightarrow$ regressione<sup>‚ôò</sup>
    - binari, <span style="color:GreenYellow">$y_i$</span> $\in \\{+1, -1\\} \rightarrow$ classificazione binaria<sup>‚ôî</sup>
    - multipli, <span style="color:GreenYellow">$y_i$</span> $\in \{0, 1, \dots, k\} \rightarrow$ classificazione multi-classe<sup>‚ôó</sup>

<small>
    <ul class="no-bullets">
        <li>‚ôï <em>Supervised Machine Learning</em></li>
        <li>‚ôò <em>Regression (focus di questa lezione)</em></li>
        <li>‚ôî <em>Binary classification (focus della Lezione 7)</em></li>
        <li>‚ôó <em>Multiclass classification (focus della Lezione 7)</em></li>
    </ul>
</small>
---

Ma quindi, cosa significa <span style="color:#FF7F50">**imparare**</span> dai dati?

--
#####  _Problemi supervisionati_

Trovare <span style="color:dodgerblue">$f(\mathbb{x})$</span> che approssimi _bene_ <span style="color:GreenYellow">$y$</span>
$$\text{argmin}_{f\in\mathcal{F}}\frac{1}{n}\sum^n _{i=1}L(f(\mathbb{x}_i), y_i)$$

<ul>
    <li>$n \rightarrow$ numero di esempi nel dataset
    <li><span style="color:dodgerblue">$f~$</span>$\rightarrow$ modello, predice il <span style="color:GreenYellow">target</span></li>
    <li><span style="color:Gold">$L~$</span>$\rightarrow$ <em>loss function</em>, misura bont√† predizione</li>
</ul>

--
##### Regressione Lineare

$$f(\mathbf{x_i}) = w^0 + w^1 x_i^1 + w^2 x_i^2 + \dots + w^d x_i^d = \hat{y_i}$$

<ul>
    <li><span style="color:dodgerblue">$\mathbf{x}_i$</span>$~\rightarrow$ $i$-esimo sample</li>
    <li>$d~\rightarrow$ numero di <em>features</em> del dataset</li>
    <li><span style="color:#FF7F50">$w^j$</span>$~\rightarrow$ parametri del modello lineare</li>
    <li><span style="color:#FF7F50">$w^0$</span>$~\rightarrow$ intercetta (aka <em>bias</em>)</li>
    <li><span style="color:GreenYellow">$\hat{y_i}$</span>$~\rightarrow$ $i$-esima predizione</li>
</ul>

<span style="color:#FF7F50"><b>imparare</b></span> dati dati $\rightarrow$ stimare il _miglior_ valore di <span style="color:#FF7F50">$w^j$</span>

--
###### <span style="color:#FF7F50">Training</span>

$$\text{argmin}_{f\in\mathcal{F}}\frac{1}{n}\sum^n _{i=1}L(f(\mathbb{x}_i), y_i)$$

$$\downarrow$$

$$\text{argmin}_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{n}\sum^n _{i=1}\big(w^0 + \sum^d _{j=1} w^j x_i^j - y_i \big)^2$$

--
##### <span style="color:#FF7F50">Training</span>

Per trovare i migliori <span style="color:#FF7F50">$w^j$</span>:
<ul>
    <li>risolviamo il problema in <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">forma chiusa</a><sup>‚ôò</sup></li>
    <li>procediamo con un <a href="https://en.wikipedia.org/wiki/Gradient_descent">algoritmo iterativo</a><sup>‚ôî</sup></li>
</ul>
</br>
</br>
<small>
    <ul class="no-bullets">
        <li>‚ôò NB: possiamo <em>quasi solo in questo caso</em></li>
        <li>‚ôî Ne esistono anche per problemi <em>non convessi</em></li>
    </ul>
</small>
--

Inserire esempio grafico qui

---

#### Quali ostacoli troveremo?

- Pochi esempi
- Feature non rappresentative
- Preparazione del dataset
- <span style="color:Coral"><b>Underfitting</b></span> _vs_ <span style="color:FireBrick"><b>Overfitting</b></span> ‚ö†Ô∏è

<small>üêç <em>Focus</em> `Jupyter`</small>
--

<img src="./assets/06/jupyter_logo.png" height="400"/>

---

### References

 <div class="twocolumn">
    <div>
        <img src="./assets/06/book1.png" height="400"/>
        </br>
        <small>
            The Elements of Statistical Learning
            </br>
            <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Springer</a>
        </small>
    </div>
    <div>
        <img src="./assets/06/book2.jpg" height="400"/>
        </br>
        <small>
            Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
            </br>
            <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">O'REILLY</a>
        </small>
    </div>
</div>