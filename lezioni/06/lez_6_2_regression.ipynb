{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esempi sintetici\n",
    "## Regressione lineare\n",
    "Andiamo a generare e **visualizzare** un semplice dataset monodimensionale, con relazione lineare tra feature e target:\n",
    "\n",
    "$$\n",
    "    f(x) = w_0 + w \\cdot x + \\varepsilon = y\n",
    "$$\n",
    "\n",
    "dove:\n",
    "\n",
    "* $w = \\sqrt{2} \\rightarrow$ parametro del modello lineare\n",
    "* $w_0 = -3 \\rightarrow$ intercetta (aka bias)\n",
    "* $x \\in \\mathcal{U}[-10, 10] \\rightarrow$ unica feature osservata\n",
    "* $y \\rightarrow$ variabile target\n",
    "* $\\varepsilon \\sim \\mathcal{N}(0, 3)$ è una modellazione di [rumore additivo gaussiano](https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "low = -10\n",
    "high = 10\n",
    "\n",
    "data = pd.DataFrame({'x': (high - low) * np.random.random_sample(n_samples) + low})\n",
    "w = np.array([-3, np.sqrt(2)])\n",
    "noise = 3 * np.random.randn(n_samples)\n",
    "\n",
    "def f(x, w):\n",
    "    return w[0] + w[1]*x\n",
    "\n",
    "data['y'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ora a campionare uniformemente il dominio di $x$, e calcoliamone la $y$ associata (teorica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.DataFrame({'x': np.linspace(data['x'].min(), data['x'].max())})\n",
    "clean_data['y'] = ...\n",
    "\n",
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label=f'f(x) = -3 + √2 x', color='C1', ax=ax)\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nostro scopo sarà quello di **quantificare** i parametri del modello ($w \\approx \\sqrt{2}$ e $w_0 \\approx -3$), a partire dai dati. Per fare ciò sfrutteremo una library di ML molto potente: [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = ...\n",
    "\n",
    "print(f\"w_0: stima {model.intercept_[0]} | vero {w[0]}\")\n",
    "print(f\"w: stima {model.coef_[0][0]} | vero {w[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo quindi ad applicare il nostro modello sul dominio della $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad aggiungere il nostro modello al plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label=f'f(x) = -3 + √2 x', color='C1', ax=ax)\n",
    "plt.plot(clean_data[['x']].values, y_pred, '--', label=r\"$\\hat{f}$\" + f\"(x) = {model.intercept_[0]:.2f} + {model.coef_[0][0]:.2f} x\", color='C2')\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quando la regressione lineare non basta?\n",
    "Ripetiamo l'esempio precedente, ma introducendo una relazione non lineare tra input ed output.\n",
    "\n",
    "$$\n",
    "    f(x) = w_0 + \\cos(\\pi w \\cdot x) + \\varepsilon = y\n",
    "$$\n",
    "\n",
    "dove:\n",
    "* $w = \\sqrt{2} \\rightarrow$ parametro del modello non lineare\n",
    "* $w_0 = -3 \\rightarrow$ intercetta (aka bias)\n",
    "* $x \\in \\mathcal{U}[0, 2] \\rightarrow$ unica feature osservata (⚠️ dominio differente rispetto a prima)\n",
    "* $y \\rightarrow$ variabile target\n",
    "* $\\varepsilon \\sim \\mathcal{N}(0, 0.33)$ è una modellazione di [rumore additivo gaussiano](https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, w):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'x': 2 * np.random.rand(n_samples)})\n",
    "w = np.array([-3, np.sqrt(2)])\n",
    "noise =  .33 * np.random.randn(n_samples)\n",
    "\n",
    "data['y'] = ...\n",
    "\n",
    "clean_data = pd.DataFrame({'x': np.linspace(data['x'].min(), data['x'].max())})\n",
    "clean_data['y'] = ...\n",
    "\n",
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label='f(x) = -3 + cos(π √2 x)', color='C1', ax=ax)\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripetiamo i passi precedenti, ed applichiamo un modello lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "y_pred = ...\n",
    "\n",
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label='f(x) = -3 + cos(π √2 x)', color='C1', ax=ax)\n",
    "plt.plot(clean_data['x'].values, y_pred, '--', color='C2', label=f'f(x) = {model.intercept_[0]:.2f} + {model.coef_[0][0]:.2f} x')\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello non approssima correttamente la relazione input/output: siamo in un caso di _underfit_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiamo il punto di vista\n",
    "Invece di cercare di approssimare la relazione input/output con un modello parametrico, proviamo un approccio non parametrico: [decision tree](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = ...\n",
    "y_pred = ...\n",
    "\n",
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label='f(x) = -3 + cos(π √2 x)', color='C1', ax=ax)\n",
    "plt.plot(clean_data['x'].values, y_pred, '--', color='C2', label=f'decision tree')\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello insegue molto bene i dati, anche troppo: siamo in un caso di _overfit_.\n",
    "\n",
    "Trovare il giusto trade-off tra _underfit_ ed _overfit_ è detto **model tuning**, ed è uno dei principali ostacoli che incontreremo allenando modelli di ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "y_pred = ...\n",
    "\n",
    "ax = plt.figure(dpi=100).gca()\n",
    "data.plot(x='x', y='y', kind='scatter', label='data', ax=ax)\n",
    "clean_data.plot(x='x', y='y', label='f(x) = -3 + cos(π √2 x)', color='C1', ax=ax)\n",
    "plt.plot(clean_data['x'].values, y_pred, '--', color='C2', label=f'decision tree')\n",
    "plt.xlabel('input feature')\n",
    "plt.ylabel('output target')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case reale: `MiMocko`\n",
    "**Domanda business**: fornire alert agli utenti in fase di noleggio se la batteria rischia di non reggere la durata del viaggio indicato.\n",
    "\n",
    "$$\n",
    "    \\downarrow\n",
    "$$\n",
    "\n",
    "**ML task**: allenare un modello di _regressione_ che risolva il seguente problema: `f(dati di viaggio) ≈ carica batteria termine`.\n",
    "\n",
    "## Caricamento e preparazione dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viaggi = pd.read_csv(\n",
    "    f'{path_to_file}/viaggi.csv',\n",
    "    sep='*',\n",
    "    decimal=','\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viaggi.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selezioniamo solo le colonne _interessanti_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ...\n",
    "data = viaggi[columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abbiamo presenza di valori mancanti?\n",
    "Se sì, scegliamo di eliminare le righe corrispondenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(data).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(BONUS)** per gestire valori mancanti è anche possibile utilizzare il modulo [`sklearn.impute`](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "## Preprocessiamo i dati e prepariamo il nostro learning set\n",
    "Di che tipo sono le colonne estratte? Sono pronte per essere analizzate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessiamo i dati, colonna per colonna\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration (minimale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca()\n",
    "data.hist(ax=ax)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separiamo le _features_ dal _target_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ...\n",
    "features = ...\n",
    "\n",
    "print(target)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = data[features]\n",
    "dfy = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "🔥 Fase **fondamentale** per capire quanto bene il nostro modello possa funzionare in produzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.33)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto sono grandi i due set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "### Training\n",
    "Alleniamo un modello linere ed otteniamo una rappresentazione del tipo:\n",
    "\n",
    "`target = bias + w_1 * feat_1 + w_2 * feat_2 + ... `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = ...\n",
    "\n",
    "# Stampiamo l'equazione del modello sotto forma di stringa\n",
    "equation = ...\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "Analizziamo il Mean Absolute Error ([MAE](https://en.wikipedia.org/wiki/Mean_absolute_error)) ed il Coefficient of determination ([R2](https://en.wikipedia.org/wiki/Coefficient_of_determination))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_train_pred = ...\n",
    "\n",
    "print('Train scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_train, y_train_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "\n",
    "print('Test scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosa possiamo concludere?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "### Training\n",
    "\n",
    "Alleniamo il modello ed osserviamo le regole imparate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = ...\n",
    "\n",
    "# Stampiamo le regole imparate del modello sotto forma di stringa\n",
    "n_rows = 15\n",
    "print('\\n'.join(tree.export_text(model, feature_names=features).split('\\n')[:n_rows]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(BONUS)** per alberi/dataset _relativamente_ piccoli, è possibile ottenere una rappresentazione grafica con [`dtreeviz`](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "```python\n",
    "from dtreeviz.trees import dtreeviz\n",
    "\n",
    "viz = dtreeviz(model, X_train, y_train,\n",
    "               target_name=target,\n",
    "               feature_names=features)\n",
    "\n",
    "viz.save(\"decision_tree.svg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ...\n",
    "\n",
    "print('Train scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_train, y_train_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "\n",
    "print('Test scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosa possiamo concludere?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Bonus) Hyperparameter tuning\n",
    "Al contrario della semplice regressione lineare, l'albero decisionale ha un parametro importante: la profondità. Scriviamo una pipeline automatica per ottimizzarne il valore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = RandomizedSearchCV(estimator=tree.DecisionTreeRegressor(),\n",
    "                           param_distributions={'max_depth': stats.randint(2, 50)},\n",
    "                           n_iter=10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "print('Train scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_train, y_train_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Test scores')\n",
    "print(f\"MAE = {metrics.mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"R2 = {metrics.r2_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
